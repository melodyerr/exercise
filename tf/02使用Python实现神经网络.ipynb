{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tensorflow训练MNIST数据\n",
    "\n",
    "本文来自 MachineLP 的CSDN 博客 ，全文地址请点击：https://blog.csdn.net/u014365862/article/details/53868414?utm_source=copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp\\train-images-idx3-ubyte.gz\n",
      "Extracting tmp\\train-labels-idx1-ubyte.gz\n",
      "Extracting tmp\\t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0  :  1896798.517791748\n",
      "1  :  2302047.776298523\n",
      "2  :  2516285.4165098667\n",
      "3  :  2642713.6947871447\n",
      "4  :  2719576.356434278\n",
      "5  :  2771963.5917451903\n",
      "6  :  2808881.215679638\n",
      "7  :  2835602.1248188126\n",
      "8  :  2863549.6765552307\n",
      "9  :  2887233.6756611164\n",
      "10  :  2905225.0687903496\n",
      "11  :  2925321.4267846975\n",
      "12  :  2946250.4993114355\n",
      "准确率:  0.9592\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow自带了MNIST数据集  \n",
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "# 下载mnist数据集  \n",
    "mnist = input_data.read_data_sets('tmp', one_hot=True)  \n",
    "# 数字(label)只能是0-9，神经网络使用10个出口节点就可以编码表示0-9；  \n",
    "#  1 -> [0,1.0,0,0,0,0,0,0,0]   one_hot表示只有一个出口节点是hot  \n",
    "#  2 -> [0,0.1,0,0,0,0,0,0,0]  \n",
    "#  5 -> [0,0,0,0,0,1.0,0,0,0]  \n",
    "#  /tmp是macOS的临时目录，重启系统数据丢失; Linux的临时目录也是/tmp  \n",
    "   \n",
    "# 定义每个层有多少'神经元''  \n",
    "n_input_layer = 28*28  # 输入层  \n",
    "   \n",
    "n_layer_1 = 500     # hide layer  \n",
    "n_layer_2 = 1000    # hide layer  \n",
    "n_layer_3 = 300     # hide layer(隐藏层)听着很神秘，其实就是除输入输出层外的中间层  \n",
    "   \n",
    "n_output_layer = 10   # 输出层  \n",
    "\"\"\" \n",
    "层数的选择：线性数据使用1层，非线性数据使用2册, 超级非线性使用3+册。层数／神经元过多会导致过拟合 \n",
    "\"\"\"  \n",
    "   \n",
    "# 定义待训练的神经网络(feedforward)  \n",
    "def neural_network(data):  \n",
    "    # 定义第一层\"神经元\"的权重和biases  \n",
    "    layer_1_w_b = {'w_':tf.Variable(tf.random_normal([n_input_layer, n_layer_1])), 'b_':tf.Variable(tf.random_normal([n_layer_1]))}  \n",
    "    # 定义第二层\"神经元\"的权重和biases  \n",
    "    layer_2_w_b = {'w_':tf.Variable(tf.random_normal([n_layer_1, n_layer_2])), 'b_':tf.Variable(tf.random_normal([n_layer_2]))}  \n",
    "    # 定义第三层\"神经元\"的权重和biases  \n",
    "    layer_3_w_b = {'w_':tf.Variable(tf.random_normal([n_layer_2, n_layer_3])), 'b_':tf.Variable(tf.random_normal([n_layer_3]))}  \n",
    "    # 定义输出层\"神经元\"的权重和biases  \n",
    "    layer_output_w_b = {'w_':tf.Variable(tf.random_normal([n_layer_3, n_output_layer])), 'b_':tf.Variable(tf.random_normal([n_output_layer]))}  \n",
    "   \n",
    "    # w·x+b  \n",
    "    layer_1 = tf.add(tf.matmul(data, layer_1_w_b['w_']), layer_1_w_b['b_'])  \n",
    "    layer_1 = tf.nn.relu(layer_1)  # 激活函数  \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, layer_2_w_b['w_']), layer_2_w_b['b_'])  \n",
    "    layer_2 = tf.nn.relu(layer_2 ) # 激活函数  \n",
    "    layer_3 = tf.add(tf.matmul(layer_2, layer_3_w_b['w_']), layer_3_w_b['b_'])  \n",
    "    layer_3 = tf.nn.relu(layer_3 ) # 激活函数  \n",
    "    layer_output = tf.add(tf.matmul(layer_3, layer_output_w_b['w_']), layer_output_w_b['b_'])  \n",
    "   \n",
    "    return layer_output  \n",
    "   \n",
    "# 每次使用100条数据进行训练  \n",
    "batch_size = 100  \n",
    "   \n",
    "X = tf.placeholder('float', [None, 28*28])   \n",
    "#[None, 28*28]代表数据数据的高和宽（矩阵），好处是如果数据不符合宽高，tensorflow会报错，不指定也可以。  \n",
    "Y = tf.placeholder('float')  \n",
    "# 使用数据训练神经网络  \n",
    "def train_neural_network(X, Y):  \n",
    "    predict = neural_network(X)  \n",
    "    cost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=Y))  \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost_func)  # learning rate 默认 0.001   \n",
    "   \n",
    "    epochs = 13  \n",
    "    with tf.Session() as session:  \n",
    "        session.run(tf.initialize_all_variables())  \n",
    "        epoch_loss = 0  \n",
    "        for epoch in range(epochs):  \n",
    "            for i in range( int(mnist.train.num_examples/batch_size) ):  \n",
    "                x, y = mnist.train.next_batch(batch_size)  \n",
    "                _, c = session.run([optimizer, cost_func], feed_dict={X:x,Y:y})  \n",
    "                epoch_loss += c  \n",
    "            print(epoch, ' : ', epoch_loss)  \n",
    "   \n",
    "                #print(predict.eval(feed_dict={X:[features]}))  \n",
    "        correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))  \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct,'float'))  \n",
    "        print('准确率: ', accuracy.eval({X:mnist.test.images, Y:mnist.test.labels}))  \n",
    "        \n",
    "train_neural_network(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
